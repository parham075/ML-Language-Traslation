{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Language Translation from English To Persian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input,LSTM,Dense\n",
    "import numpy as np\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE= 64\n",
    "EPOCHS=100\n",
    "LATENT_DIM=256 # latent dimensionality of encoding space\n",
    "NUM_SAMPLES=2600 # number of samples to train\n",
    "DATA_PATH='./pes.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the data\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "with open(DATA_PATH,'r',encoding='utf-8') as file:\n",
    "  lines = file.read().split('\\n')\n",
    "for line in lines[:min(NUM_SAMPLES,len(lines)-1)]:\n",
    "  input_text, target_text , _ = line.split('\\t')\n",
    "  # The user use \"tab\" as the \"start sequence\" character\n",
    "  # for the targets, and \"\\n\" as \"end of sequence\" charecter\n",
    "  target_text = '\\t' + target_text + \"\\n\"\n",
    "  input_texts.append(input_text)\n",
    "  target_texts.append(target_text)\n",
    "  for char in input_text:\n",
    "    if char not in input_characters:\n",
    "      input_characters.add(char)\n",
    "  for char in target_text:\n",
    "    if char not in target_characters:\n",
    "      target_characters.add(char)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-04-06 02:38:33.552\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mNumber of samples: 2600\u001b[0m\n",
      "\u001b[32m2024-04-06 02:38:33.554\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1mNumber of unique input tokens: 67\u001b[0m\n",
      "\u001b[32m2024-04-06 02:38:33.555\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m10\u001b[0m - \u001b[1mNumber of unique target tokens: 75\u001b[0m\n",
      "\u001b[32m2024-04-06 02:38:33.555\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mMax sequence length for input: 42\u001b[0m\n",
      "\u001b[32m2024-04-06 02:38:33.556\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mMax sequence length for target: 65\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# extract some properties\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "logger.info(f\"Number of samples: {len(input_texts)}\")\n",
    "logger.info(f\"Number of unique input tokens: {num_encoder_tokens}\", )\n",
    "logger.info(f\"Number of unique target tokens: {num_decoder_tokens}\")\n",
    "logger.info(f\"Max sequence length for input: {max_encoder_seq_length}\")\n",
    "logger.info(f\"Max sequence length for target: {max_decoder_seq_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict(\n",
    "  [(char,i) for i,char in enumerate(input_characters)]\n",
    ")\n",
    "target_token_index = dict(\n",
    "  [(char,i) for i,char in enumerate(target_characters)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one hot representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros(\n",
    "  (len(input_texts), max_encoder_seq_length,num_encoder_tokens),\n",
    "  dtype=\"float32\"\n",
    ")\n",
    "decoder_input_data = np.zeros(\n",
    "  (len(input_texts), max_decoder_seq_length,num_decoder_tokens),\n",
    "    dtype=\"float32\"\n",
    ")\n",
    "decoder_target_data = np.zeros(\n",
    "  (len(target_texts), max_decoder_seq_length,num_decoder_tokens),\n",
    "  dtype=\"float32\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i , (input_text, target_text) in enumerate(zip(input_texts,target_texts)):\n",
    "  for t , char in enumerate(input_text):\n",
    "    encoder_input_data[i,t,input_token_index[char]]=1.\n",
    "  encoder_input_data[i, t+1:, input_token_index[' ']]=1.\n",
    "  for t, char in enumerate(target_text):\n",
    "    # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "    decoder_input_data[i, t , target_token_index[char]]= 1.\n",
    "    if t > 0:\n",
    "      # decoder_target_data will be ahead by one timestep\n",
    "      # and will not include the start character\n",
    "      decoder_target_data[i, t-1, target_token_index[char]]=1.\n",
    "  decoder_input_data[i, t+1:, target_token_index[' ']] = 1.\n",
    "  decoder_target_data[i, t:, target_token_index[' ']] = 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm_model:\n",
    "  def __init__(self,\n",
    "               num_encoder_tokens,num_decoder_tokens,\n",
    "               latent_dim, encoder_input_data,\n",
    "               decoder_input_data, decoder_target_data,\n",
    "               batch_size, epochs\n",
    "               ) -> None:\n",
    "    self.num_encoder_tokens = num_encoder_tokens\n",
    "    self.num_decoder_tokens = num_decoder_tokens\n",
    "    self.latent_dim = latent_dim\n",
    "    self.encoder_input_data = encoder_input_data\n",
    "    self.decoder_input_data = decoder_input_data\n",
    "    self.decoder_target_data = decoder_target_data\n",
    "    self.batch_size = batch_size\n",
    "    self.epochs = epochs\n",
    "  def encoder(self):\n",
    "    self.encoder_inputs = Input(shape=(None, self.num_encoder_tokens))\n",
    "    encoder = LSTM(self.latent_dim, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder(self.encoder_inputs)\n",
    "    self.encoder_states = [state_h,state_c]\n",
    "  def decoder(self):\n",
    "    self.decoder_inputs = Input(shape=(None,self.num_decoder_tokens))\n",
    "    decoder = LSTM(self.latent_dim,return_sequences=True,return_state=True)\n",
    "    self.decoder_outputs,_,_ = decoder(self.decoder_inputs, initial_state=self.encoder_states)\n",
    "    decoder_dense = Dense(self.num_decoder_tokens,activation=\"softmax\")\n",
    "    self.decoder_outputs = decoder_dense(self.decoder_outputs)\n",
    "  def model(self):\n",
    "    self.model = Model([self.encoder_inputs,self.decoder_inputs], self.decoder_outputs)\n",
    "    self.model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "  def training(self):\n",
    "    return self.model.fit([self.encoder_input_data,self.decoder_input_data],self.decoder_target_data,\n",
    "                   batch_size = self.batch_size,\n",
    "                   epochs= self.epochs, validation_split = 0.2)\n",
    "    \n",
    "    \n",
    "translation_model_obj = lstm_model(\n",
    "                                  num_encoder_tokens=num_encoder_tokens,\n",
    "                                  num_decoder_tokens=num_decoder_tokens,\n",
    "                                  latent_dim=LATENT_DIM,\n",
    "                                  encoder_input_data= encoder_input_data,\n",
    "                                  decoder_input_data=decoder_input_data, \n",
    "                                  decoder_target_data=decoder_target_data,\n",
    "                                  batch_size=BATCH_SIZE, epochs=EPOCHS\n",
    "                                   )\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "33/33 [==============================] - 14s 372ms/step - loss: 1.8317 - accuracy: 0.6540 - val_loss: 2.0384 - val_accuracy: 0.5075\n",
      "Epoch 2/100\n",
      "33/33 [==============================] - 11s 327ms/step - loss: 1.3874 - accuracy: 0.6745 - val_loss: 1.9986 - val_accuracy: 0.5280\n",
      "Epoch 3/100\n",
      "33/33 [==============================] - 6s 177ms/step - loss: 1.2967 - accuracy: 0.6786 - val_loss: 1.9520 - val_accuracy: 0.5280\n",
      "Epoch 4/100\n",
      "33/33 [==============================] - 6s 182ms/step - loss: 1.2583 - accuracy: 0.6789 - val_loss: 1.8466 - val_accuracy: 0.5208\n",
      "Epoch 5/100\n",
      "33/33 [==============================] - 6s 177ms/step - loss: 1.2205 - accuracy: 0.6774 - val_loss: 2.0614 - val_accuracy: 0.5280\n",
      "Epoch 6/100\n",
      "33/33 [==============================] - 6s 177ms/step - loss: 1.1937 - accuracy: 0.6793 - val_loss: 1.8037 - val_accuracy: 0.4975\n",
      "Epoch 7/100\n",
      "33/33 [==============================] - 6s 175ms/step - loss: 1.1616 - accuracy: 0.6811 - val_loss: 1.7562 - val_accuracy: 0.5135\n",
      "Epoch 8/100\n",
      "33/33 [==============================] - 6s 179ms/step - loss: 1.1639 - accuracy: 0.6885 - val_loss: 1.7017 - val_accuracy: 0.5399\n",
      "Epoch 9/100\n",
      "33/33 [==============================] - 6s 183ms/step - loss: 1.1145 - accuracy: 0.6922 - val_loss: 1.6718 - val_accuracy: 0.5417\n",
      "Epoch 10/100\n",
      "33/33 [==============================] - 6s 180ms/step - loss: 1.1112 - accuracy: 0.6991 - val_loss: 1.6331 - val_accuracy: 0.5472\n",
      "Epoch 11/100\n",
      "33/33 [==============================] - 6s 184ms/step - loss: 1.0530 - accuracy: 0.7071 - val_loss: 1.5843 - val_accuracy: 0.5494\n",
      "Epoch 12/100\n",
      "33/33 [==============================] - 6s 182ms/step - loss: 1.0249 - accuracy: 0.7165 - val_loss: 1.5492 - val_accuracy: 0.5742\n",
      "Epoch 13/100\n",
      "33/33 [==============================] - 10s 313ms/step - loss: 0.9960 - accuracy: 0.7263 - val_loss: 1.5015 - val_accuracy: 0.5891\n",
      "Epoch 14/100\n",
      "33/33 [==============================] - 10s 307ms/step - loss: 1.0365 - accuracy: 0.7122 - val_loss: 1.5026 - val_accuracy: 0.5851\n",
      "Epoch 15/100\n",
      "33/33 [==============================] - 10s 318ms/step - loss: 0.9829 - accuracy: 0.7300 - val_loss: 2.9689 - val_accuracy: 0.5364\n",
      "Epoch 16/100\n",
      "33/33 [==============================] - 10s 314ms/step - loss: 0.9700 - accuracy: 0.7356 - val_loss: 1.4572 - val_accuracy: 0.5971\n",
      "Epoch 17/100\n",
      "33/33 [==============================] - 11s 319ms/step - loss: 0.9427 - accuracy: 0.7382 - val_loss: 1.4400 - val_accuracy: 0.6020\n",
      "Epoch 18/100\n",
      "33/33 [==============================] - 10s 314ms/step - loss: 0.9368 - accuracy: 0.7406 - val_loss: 1.4366 - val_accuracy: 0.6033\n",
      "Epoch 19/100\n",
      "33/33 [==============================] - 11s 320ms/step - loss: 0.9271 - accuracy: 0.7434 - val_loss: 1.4827 - val_accuracy: 0.5779\n",
      "Epoch 20/100\n",
      "33/33 [==============================] - 10s 306ms/step - loss: 0.9186 - accuracy: 0.7443 - val_loss: 1.4357 - val_accuracy: 0.5960\n",
      "Epoch 21/100\n",
      "33/33 [==============================] - 10s 315ms/step - loss: 0.9160 - accuracy: 0.7460 - val_loss: 1.3967 - val_accuracy: 0.6129\n",
      "Epoch 22/100\n",
      "33/33 [==============================] - 10s 313ms/step - loss: 0.9063 - accuracy: 0.7471 - val_loss: 1.4176 - val_accuracy: 0.6058\n",
      "Epoch 23/100\n",
      "33/33 [==============================] - 10s 311ms/step - loss: 0.8984 - accuracy: 0.7493 - val_loss: 1.3864 - val_accuracy: 0.6150\n",
      "Epoch 24/100\n",
      "33/33 [==============================] - 10s 307ms/step - loss: 0.8934 - accuracy: 0.7510 - val_loss: 1.3822 - val_accuracy: 0.6165\n",
      "Epoch 25/100\n",
      "33/33 [==============================] - 10s 311ms/step - loss: 0.8923 - accuracy: 0.7515 - val_loss: 1.3853 - val_accuracy: 0.6137\n",
      "Epoch 26/100\n",
      "33/33 [==============================] - 10s 320ms/step - loss: 0.8819 - accuracy: 0.7531 - val_loss: 1.3740 - val_accuracy: 0.6207\n",
      "Epoch 27/100\n",
      "33/33 [==============================] - 10s 318ms/step - loss: 0.8779 - accuracy: 0.7540 - val_loss: 1.3758 - val_accuracy: 0.6144\n",
      "Epoch 28/100\n",
      "33/33 [==============================] - 10s 311ms/step - loss: 0.8722 - accuracy: 0.7556 - val_loss: 1.3688 - val_accuracy: 0.6193\n",
      "Epoch 29/100\n",
      "33/33 [==============================] - 10s 310ms/step - loss: 0.8676 - accuracy: 0.7568 - val_loss: 1.3515 - val_accuracy: 0.6232\n",
      "Epoch 30/100\n",
      "33/33 [==============================] - 11s 335ms/step - loss: 0.8625 - accuracy: 0.7594 - val_loss: 1.3708 - val_accuracy: 0.6182\n",
      "Epoch 31/100\n",
      "33/33 [==============================] - 7s 214ms/step - loss: 0.8584 - accuracy: 0.7603 - val_loss: 1.3542 - val_accuracy: 0.6218\n",
      "Epoch 32/100\n",
      "33/33 [==============================] - 11s 321ms/step - loss: 0.8522 - accuracy: 0.7614 - val_loss: 1.3415 - val_accuracy: 0.6221\n",
      "Epoch 33/100\n",
      "33/33 [==============================] - 11s 322ms/step - loss: 0.8503 - accuracy: 0.7618 - val_loss: 1.3471 - val_accuracy: 0.6280\n",
      "Epoch 34/100\n",
      "33/33 [==============================] - 10s 319ms/step - loss: 0.8451 - accuracy: 0.7634 - val_loss: 1.3507 - val_accuracy: 0.6245\n",
      "Epoch 35/100\n",
      "33/33 [==============================] - 10s 313ms/step - loss: 0.8396 - accuracy: 0.7653 - val_loss: 1.3478 - val_accuracy: 0.6266\n",
      "Epoch 36/100\n",
      "33/33 [==============================] - 11s 321ms/step - loss: 0.8359 - accuracy: 0.7660 - val_loss: 1.3205 - val_accuracy: 0.6342\n",
      "Epoch 37/100\n",
      "33/33 [==============================] - 10s 319ms/step - loss: 0.8894 - accuracy: 0.7582 - val_loss: 1.3204 - val_accuracy: 0.6331\n",
      "Epoch 38/100\n",
      "33/33 [==============================] - 10s 298ms/step - loss: 0.8272 - accuracy: 0.7687 - val_loss: 1.3263 - val_accuracy: 0.6322\n",
      "Epoch 39/100\n",
      "33/33 [==============================] - 10s 312ms/step - loss: 0.8264 - accuracy: 0.7695 - val_loss: 1.3161 - val_accuracy: 0.6364\n",
      "Epoch 40/100\n",
      "33/33 [==============================] - 10s 318ms/step - loss: 0.8230 - accuracy: 0.7703 - val_loss: 1.3159 - val_accuracy: 0.6357\n",
      "Epoch 41/100\n",
      "33/33 [==============================] - 10s 313ms/step - loss: 0.8202 - accuracy: 0.7711 - val_loss: 1.3025 - val_accuracy: 0.6375\n",
      "Epoch 42/100\n",
      "33/33 [==============================] - 11s 320ms/step - loss: 0.8143 - accuracy: 0.7725 - val_loss: 1.3014 - val_accuracy: 0.6396\n",
      "Epoch 43/100\n",
      "33/33 [==============================] - 10s 305ms/step - loss: 0.8097 - accuracy: 0.7739 - val_loss: 1.3156 - val_accuracy: 0.6395\n",
      "Epoch 44/100\n",
      "33/33 [==============================] - 10s 318ms/step - loss: 0.8067 - accuracy: 0.7757 - val_loss: 1.3041 - val_accuracy: 0.6393\n",
      "Epoch 45/100\n",
      "33/33 [==============================] - 11s 320ms/step - loss: 0.8032 - accuracy: 0.7755 - val_loss: 1.2926 - val_accuracy: 0.6393\n",
      "Epoch 46/100\n",
      "33/33 [==============================] - 11s 322ms/step - loss: 0.7991 - accuracy: 0.7761 - val_loss: 1.3120 - val_accuracy: 0.6374\n",
      "Epoch 47/100\n",
      "33/33 [==============================] - 10s 317ms/step - loss: 0.7953 - accuracy: 0.7778 - val_loss: 1.2771 - val_accuracy: 0.6494\n",
      "Epoch 48/100\n",
      "33/33 [==============================] - 10s 320ms/step - loss: 0.7921 - accuracy: 0.7782 - val_loss: 1.2964 - val_accuracy: 0.6444\n",
      "Epoch 49/100\n",
      "33/33 [==============================] - 10s 315ms/step - loss: 0.7881 - accuracy: 0.7800 - val_loss: 1.2752 - val_accuracy: 0.6492\n",
      "Epoch 50/100\n",
      "33/33 [==============================] - 11s 321ms/step - loss: 0.7842 - accuracy: 0.7805 - val_loss: 1.2761 - val_accuracy: 0.6468\n",
      "Epoch 51/100\n",
      "33/33 [==============================] - 10s 317ms/step - loss: 0.7813 - accuracy: 0.7813 - val_loss: 1.2779 - val_accuracy: 0.6494\n",
      "Epoch 52/100\n",
      "33/33 [==============================] - 10s 320ms/step - loss: 0.7767 - accuracy: 0.7824 - val_loss: 1.2843 - val_accuracy: 0.6467\n",
      "Epoch 53/100\n",
      "33/33 [==============================] - 10s 310ms/step - loss: 0.7736 - accuracy: 0.7836 - val_loss: 1.2671 - val_accuracy: 0.6519\n",
      "Epoch 54/100\n",
      "33/33 [==============================] - 11s 320ms/step - loss: 0.7700 - accuracy: 0.7849 - val_loss: 1.2686 - val_accuracy: 0.6473\n",
      "Epoch 55/100\n",
      "33/33 [==============================] - 11s 320ms/step - loss: 0.7669 - accuracy: 0.7860 - val_loss: 1.2620 - val_accuracy: 0.6498\n",
      "Epoch 56/100\n",
      "33/33 [==============================] - 11s 328ms/step - loss: 0.7683 - accuracy: 0.7855 - val_loss: 1.2782 - val_accuracy: 0.6476\n",
      "Epoch 57/100\n",
      "33/33 [==============================] - 10s 318ms/step - loss: 0.7604 - accuracy: 0.7875 - val_loss: 1.2643 - val_accuracy: 0.6505\n",
      "Epoch 58/100\n",
      "33/33 [==============================] - 10s 319ms/step - loss: 0.7561 - accuracy: 0.7886 - val_loss: 1.2499 - val_accuracy: 0.6557\n",
      "Epoch 59/100\n",
      "33/33 [==============================] - 10s 315ms/step - loss: 0.7533 - accuracy: 0.7895 - val_loss: 1.2699 - val_accuracy: 0.6508\n",
      "Epoch 60/100\n",
      "33/33 [==============================] - 11s 321ms/step - loss: 0.7492 - accuracy: 0.7903 - val_loss: 1.2670 - val_accuracy: 0.6544\n",
      "Epoch 61/100\n",
      "33/33 [==============================] - 10s 320ms/step - loss: 0.7450 - accuracy: 0.7918 - val_loss: 1.2593 - val_accuracy: 0.6499\n",
      "Epoch 62/100\n",
      "33/33 [==============================] - 11s 325ms/step - loss: 0.7431 - accuracy: 0.7916 - val_loss: 1.2509 - val_accuracy: 0.6537\n",
      "Epoch 63/100\n",
      "33/33 [==============================] - 10s 293ms/step - loss: 0.7394 - accuracy: 0.7933 - val_loss: 1.2502 - val_accuracy: 0.6570\n",
      "Epoch 64/100\n",
      "33/33 [==============================] - 11s 321ms/step - loss: 0.7353 - accuracy: 0.7947 - val_loss: 1.2360 - val_accuracy: 0.6578\n",
      "Epoch 65/100\n",
      "33/33 [==============================] - 6s 180ms/step - loss: 0.7356 - accuracy: 0.7940 - val_loss: 1.2390 - val_accuracy: 0.6594\n",
      "Epoch 66/100\n",
      "33/33 [==============================] - 5s 166ms/step - loss: 0.7281 - accuracy: 0.7961 - val_loss: 1.2439 - val_accuracy: 0.6565\n",
      "Epoch 67/100\n",
      "33/33 [==============================] - 6s 168ms/step - loss: 0.7266 - accuracy: 0.7972 - val_loss: 1.2332 - val_accuracy: 0.6620\n",
      "Epoch 68/100\n",
      "33/33 [==============================] - 5s 166ms/step - loss: 0.7219 - accuracy: 0.7978 - val_loss: 1.2522 - val_accuracy: 0.6550\n",
      "Epoch 69/100\n",
      "33/33 [==============================] - 5s 163ms/step - loss: 0.7188 - accuracy: 0.7992 - val_loss: 1.2267 - val_accuracy: 0.6620\n",
      "Epoch 70/100\n",
      "33/33 [==============================] - 6s 169ms/step - loss: 0.7174 - accuracy: 0.7989 - val_loss: 1.2460 - val_accuracy: 0.6574\n",
      "Epoch 71/100\n",
      "33/33 [==============================] - 5s 165ms/step - loss: 0.7255 - accuracy: 0.7946 - val_loss: 1.2426 - val_accuracy: 0.6583\n",
      "Epoch 72/100\n",
      "33/33 [==============================] - 5s 163ms/step - loss: 0.7180 - accuracy: 0.7971 - val_loss: 1.2466 - val_accuracy: 0.6578\n",
      "Epoch 73/100\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.7132 - accuracy: 0.8000 - val_loss: 1.2578 - val_accuracy: 0.6585\n",
      "Epoch 74/100\n",
      "33/33 [==============================] - 5s 166ms/step - loss: 0.7094 - accuracy: 0.8007 - val_loss: 1.2223 - val_accuracy: 0.6635\n",
      "Epoch 75/100\n",
      "33/33 [==============================] - 6s 170ms/step - loss: 0.7059 - accuracy: 0.8020 - val_loss: 1.2254 - val_accuracy: 0.6642\n",
      "Epoch 76/100\n",
      "33/33 [==============================] - 5s 166ms/step - loss: 0.7003 - accuracy: 0.8039 - val_loss: 1.2333 - val_accuracy: 0.6627\n",
      "Epoch 77/100\n",
      "33/33 [==============================] - 6s 169ms/step - loss: 0.6961 - accuracy: 0.8052 - val_loss: 1.2337 - val_accuracy: 0.6604\n",
      "Epoch 78/100\n",
      "33/33 [==============================] - 6s 171ms/step - loss: 0.6914 - accuracy: 0.8066 - val_loss: 1.2357 - val_accuracy: 0.6628\n",
      "Epoch 79/100\n",
      "33/33 [==============================] - 6s 168ms/step - loss: 0.6906 - accuracy: 0.8068 - val_loss: 1.2221 - val_accuracy: 0.6678\n",
      "Epoch 80/100\n",
      "33/33 [==============================] - 5s 167ms/step - loss: 0.6849 - accuracy: 0.8080 - val_loss: 1.2239 - val_accuracy: 0.6675\n",
      "Epoch 81/100\n",
      "33/33 [==============================] - 6s 170ms/step - loss: 0.6805 - accuracy: 0.8099 - val_loss: 1.2283 - val_accuracy: 0.6643\n",
      "Epoch 82/100\n",
      "33/33 [==============================] - 6s 168ms/step - loss: 0.6763 - accuracy: 0.8107 - val_loss: 1.2294 - val_accuracy: 0.6656\n",
      "Epoch 83/100\n",
      "33/33 [==============================] - 5s 167ms/step - loss: 0.6735 - accuracy: 0.8115 - val_loss: 1.2207 - val_accuracy: 0.6676\n",
      "Epoch 84/100\n",
      "33/33 [==============================] - 6s 181ms/step - loss: 0.6698 - accuracy: 0.8126 - val_loss: 1.2476 - val_accuracy: 0.6623\n",
      "Epoch 85/100\n",
      "33/33 [==============================] - 6s 167ms/step - loss: 0.6662 - accuracy: 0.8132 - val_loss: 1.2358 - val_accuracy: 0.6674\n",
      "Epoch 86/100\n",
      "33/33 [==============================] - 6s 189ms/step - loss: 0.6625 - accuracy: 0.8145 - val_loss: 1.2278 - val_accuracy: 0.6652\n",
      "Epoch 87/100\n",
      "33/33 [==============================] - 6s 171ms/step - loss: 0.6597 - accuracy: 0.8148 - val_loss: 1.2119 - val_accuracy: 0.6686\n",
      "Epoch 88/100\n",
      "33/33 [==============================] - 5s 166ms/step - loss: 0.6555 - accuracy: 0.8160 - val_loss: 1.2327 - val_accuracy: 0.6652\n",
      "Epoch 89/100\n",
      "33/33 [==============================] - 5s 167ms/step - loss: 0.6569 - accuracy: 0.8153 - val_loss: 1.2269 - val_accuracy: 0.6679\n",
      "Epoch 90/100\n",
      "33/33 [==============================] - 6s 168ms/step - loss: 0.6524 - accuracy: 0.8170 - val_loss: 1.2347 - val_accuracy: 0.6647\n",
      "Epoch 91/100\n",
      "33/33 [==============================] - 5s 166ms/step - loss: 0.6483 - accuracy: 0.8175 - val_loss: 1.2291 - val_accuracy: 0.6674\n",
      "Epoch 92/100\n",
      "33/33 [==============================] - 5s 167ms/step - loss: 0.6466 - accuracy: 0.8182 - val_loss: 1.2325 - val_accuracy: 0.6657\n",
      "Epoch 93/100\n",
      "33/33 [==============================] - 5s 165ms/step - loss: 0.6417 - accuracy: 0.8199 - val_loss: 1.2416 - val_accuracy: 0.6615\n",
      "Epoch 94/100\n",
      "33/33 [==============================] - 5s 166ms/step - loss: 0.6370 - accuracy: 0.8210 - val_loss: 1.2418 - val_accuracy: 0.6649\n",
      "Epoch 95/100\n",
      "33/33 [==============================] - 5s 165ms/step - loss: 0.6456 - accuracy: 0.8187 - val_loss: 1.2291 - val_accuracy: 0.6668\n",
      "Epoch 96/100\n",
      "33/33 [==============================] - 5s 167ms/step - loss: 0.6394 - accuracy: 0.8201 - val_loss: 1.2246 - val_accuracy: 0.6680\n",
      "Epoch 97/100\n",
      "33/33 [==============================] - 6s 168ms/step - loss: 0.6401 - accuracy: 0.8198 - val_loss: 1.2401 - val_accuracy: 0.6660\n",
      "Epoch 98/100\n",
      "33/33 [==============================] - 5s 162ms/step - loss: 0.6341 - accuracy: 0.8214 - val_loss: 1.2636 - val_accuracy: 0.6616\n",
      "Epoch 99/100\n",
      "33/33 [==============================] - 5s 165ms/step - loss: 0.6315 - accuracy: 0.8228 - val_loss: 1.2313 - val_accuracy: 0.6655\n",
      "Epoch 100/100\n",
      "33/33 [==============================] - 6s 198ms/step - loss: 0.6298 - accuracy: 0.8232 - val_loss: 1.2342 - val_accuracy: 0.6661\n"
     ]
    }
   ],
   "source": [
    "translation_model_obj.encoder()\n",
    "translation_model_obj.decoder()\n",
    "translation_model_obj.model()\n",
    "history = translation_model_obj.training()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trnaslate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
